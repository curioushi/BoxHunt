#!/usr/bin/env python3
"""
BoxHuntä½¿ç”¨ç¤ºä¾‹
"""

import asyncio
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from boxhunt.config import Config
from boxhunt.crawler import BoxHuntCrawler


async def basic_example():
    """åŸºç¡€ä½¿ç”¨ç¤ºä¾‹"""
    print("ğŸš€ BoxHuntåŸºç¡€ç¤ºä¾‹")

    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = BoxHuntCrawler()

    # æ£€æŸ¥APIé…ç½®
    print("\n1. æ£€æŸ¥APIé…ç½®...")
    stats = crawler.get_statistics()
    available_apis = stats["api_sources"]

    if not available_apis:
        print("âŒ æ²¡æœ‰å¯ç”¨çš„APIæºï¼Œè¯·æ£€æŸ¥.envæ–‡ä»¶ä¸­çš„APIå¯†é’¥é…ç½®")
        print("   å‚è€ƒenv.exampleæ–‡ä»¶è¿›è¡Œé…ç½®")
        return

    print(f"âœ… å¯ç”¨APIæº: {', '.join(available_apis)}")

    # æµ‹è¯•APIè¿æ¥
    print("\n2. æµ‹è¯•APIè¿æ¥...")
    test_results = await crawler.test_apis()

    for source, result in test_results.items():
        if result["status"] == "success":
            print(f"âœ… {source}: {result['results_count']} ä¸ªæµ‹è¯•ç»“æœ")
        else:
            print(f"âŒ {source}: {result['error']}")

    # çˆ¬å–å°‘é‡å›¾ç‰‡è¿›è¡Œæµ‹è¯•
    print("\n3. æµ‹è¯•çˆ¬å– (æ¯ä¸ªæºæœ€å¤š5å¼ å›¾ç‰‡)...")
    result = await crawler.crawl_single_keyword(
        "cardboard box", max_images_per_source=5
    )

    print("ğŸ“Š çˆ¬å–ç»“æœ:")
    print(f"   æ‰¾åˆ°å›¾ç‰‡: {result['found_images']}")
    print(f"   å¤„ç†æˆåŠŸ: {result['processed_images']}")
    print(f"   ä¿å­˜å›¾ç‰‡: {result['saved_images']}")


async def advanced_example():
    """é«˜çº§ä½¿ç”¨ç¤ºä¾‹"""
    print("\nğŸ”¥ BoxHunté«˜çº§ç¤ºä¾‹")

    crawler = BoxHuntCrawler()

    # ä½¿ç”¨è‡ªå®šä¹‰å…³é”®è¯åˆ—è¡¨
    custom_keywords = ["cardboard box", "çº¸ç®±", "corrugated box"]

    print(f"\nä½¿ç”¨è‡ªå®šä¹‰å…³é”®è¯: {custom_keywords}")

    # æ‰¹é‡çˆ¬å–
    results = await crawler.crawl_multiple_keywords(
        keywords=custom_keywords,
        max_images_per_source=10,
        delay_between_keywords=0.5,  # å…³é”®è¯é—´éš”0.5ç§’
    )

    print("\nğŸ“ˆ æ‰¹é‡çˆ¬å–ç»“æœ:")
    print(f"   å¤„ç†å…³é”®è¯: {results['completed_keywords']}/{results['total_keywords']}")
    print(f"   æ‰¾åˆ°å›¾ç‰‡æ€»æ•°: {results['total_found_images']}")
    print(f"   ä¿å­˜å›¾ç‰‡æ€»æ•°: {results['total_saved_images']}")

    # æ˜¾ç¤ºæ¯ä¸ªå…³é”®è¯çš„è¯¦ç»†ç»“æœ
    print("\nğŸ“ è¯¦ç»†ç»“æœ:")
    for keyword_result in results["keyword_results"]:
        print(
            f"   '{keyword_result['keyword']}': {keyword_result['saved_images']} å¼ ä¿å­˜"
        )

    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    stats = crawler.get_statistics()
    storage_stats = stats["storage"]

    print("\nğŸ“Š æ”¶é›†ç»Ÿè®¡:")
    print(f"   æ€»å›¾ç‰‡æ•°: {storage_stats['total_images']}")
    print(f"   æ€»å¤§å°: {storage_stats['total_size'] / (1024 * 1024):.2f} MB")
    print(f"   å¹³å‡å°ºå¯¸: {storage_stats['avg_width']}Ã—{storage_stats['avg_height']}")

    if storage_stats["sources"]:
        print("   å„æºç»Ÿè®¡:")
        for source, count in storage_stats["sources"].items():
            print(f"     {source}: {count} å¼ ")


async def resume_example():
    """æ–­ç‚¹ç»­ä¼ ç¤ºä¾‹"""
    print("\nğŸ”„ æ–­ç‚¹ç»­ä¼ ç¤ºä¾‹")

    crawler = BoxHuntCrawler()

    print("æ¨¡æ‹Ÿæ¢å¤ä¸­æ–­çš„çˆ¬å–ä»»åŠ¡...")

    # æ¢å¤çˆ¬å– (ä¼šè‡ªåŠ¨è·³è¿‡å·²ä¸‹è½½çš„å›¾ç‰‡)
    result = await crawler.resume_crawl(
        keywords=["shipping box", "åŒ…è£…ç®±"], max_images_per_source=15
    )

    print("æ¢å¤çˆ¬å–å®Œæˆ:")
    print(f"  æ–°ä¿å­˜å›¾ç‰‡: {result['total_saved_images']}")


def management_example():
    """ç®¡ç†åŠŸèƒ½ç¤ºä¾‹"""
    print("\nğŸ› ï¸ ç®¡ç†åŠŸèƒ½ç¤ºä¾‹")

    crawler = BoxHuntCrawler()

    # è·å–ç»Ÿè®¡ä¿¡æ¯
    print("1. è·å–ç»Ÿè®¡ä¿¡æ¯...")
    stats = crawler.get_statistics()
    print(f"   å½“å‰æ”¶è—: {stats['storage']['total_images']} å¼ å›¾ç‰‡")

    # æ¸…ç†å­¤ç«‹æ–‡ä»¶
    print("\n2. æ¸…ç†å­¤ç«‹æ–‡ä»¶...")
    cleanup_result = crawler.cleanup()
    print(f"   æ¸…ç†å­¤ç«‹æ–‡ä»¶: {cleanup_result['orphaned_files_removed']}")
    print(f"   æ¸…ç†å¤±è´¥URL: {cleanup_result['failed_urls_cleared']}")

    # å¯¼å‡ºå…ƒæ•°æ®
    print("\n3. å¯¼å‡ºå…ƒæ•°æ®...")
    export_file = crawler.export_results(format="json")
    if export_file:
        print(f"   å…ƒæ•°æ®å·²å¯¼å‡ºåˆ°: {export_file}")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        print("=" * 50)
        print("   BoxHunt çº¸ç®±å›¾åƒçˆ¬è™«ç¤ºä¾‹ç¨‹åº")
        print("=" * 50)

        # æ£€æŸ¥é…ç½®
        api_keys = Config.validate_api_keys()
        if not any(api_keys.values()):
            print("\nâš ï¸  è­¦å‘Š: æ²¡æœ‰æ£€æµ‹åˆ°APIå¯†é’¥")
            print("è¯·å¤åˆ¶ env.example åˆ° .env å¹¶é…ç½®APIå¯†é’¥")
            print("\nAPIå¯†é’¥çŠ¶æ€:")
            for api, available in api_keys.items():
                status = "âœ…" if available else "âŒ"
                print(f"  {status} {api}")

            response = input("\næ˜¯å¦ç»§ç»­è¿è¡Œç¤ºä¾‹? (y/N): ")
            if response.lower() != "y":
                return

        # è¿è¡Œç¤ºä¾‹
        await basic_example()

        print("\n" + "=" * 30)
        response = input("æ˜¯å¦ç»§ç»­è¿è¡Œé«˜çº§ç¤ºä¾‹? (y/N): ")
        if response.lower() == "y":
            await advanced_example()

        print("\n" + "=" * 30)
        response = input("æ˜¯å¦è¿è¡Œæ–­ç‚¹ç»­ä¼ ç¤ºä¾‹? (y/N): ")
        if response.lower() == "y":
            await resume_example()

        print("\n" + "=" * 30)
        response = input("æ˜¯å¦è¿è¡Œç®¡ç†åŠŸèƒ½ç¤ºä¾‹? (y/N): ")
        if response.lower() == "y":
            management_example()

        print("\nğŸ‰ ç¤ºä¾‹ç¨‹åºè¿è¡Œå®Œæˆ!")
        print("\nğŸ’¡ æç¤º:")
        print("  - ä½¿ç”¨ 'boxhunt config' æŸ¥çœ‹å®Œæ•´é…ç½®")
        print("  - ä½¿ç”¨ 'boxhunt crawl --help' æŸ¥çœ‹æ‰€æœ‰é€‰é¡¹")
        print("  - æŸ¥çœ‹ data/ ç›®å½•ä¸‹çš„ä¸‹è½½æ–‡ä»¶")

    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è¿è¡Œå‡ºé”™: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())
